(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{398:function(t,e,a){t.exports=a.p+"assets/img/dnn-parameters.d1808f3f.png"},399:function(t,e,a){t.exports=a.p+"assets/img/dnn-test.eed01107.png"},432:function(t,e,a){"use strict";a.r(e);var r=a(27),s=Object(r.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"深层神经网络"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#深层神经网络"}},[t._v("#")]),t._v(" 深层神经网络")]),t._v(" "),r("p",[t._v("一般来说，有比较多的隐藏层的神经网络会比较好用。")]),t._v(" "),r("h2",{attrs:{id:"更强的特征表示能力"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#更强的特征表示能力"}},[t._v("#")]),t._v(" 更强的特征表示能力")]),t._v(" "),r("p",[t._v("深层结构的一个优点是可以增加特征的重用性，从而指数级地增加表示能力。")]),t._v(" "),r("p",[t._v("比如，对于人脸识别，神经网络的第一层从原始图片中提取人脸的轮廓和边缘，每个神经元学习到不同边缘的信息；第二层将第一层学得的边缘信息组合起来，形成人脸的一些局部的特征，例如眼睛、嘴巴等；后面的几层逐步将上一层的特征组合起来，形成人脸的模样。随着神经网络层数的增加，特征也从原来的边缘逐步扩展为人脸的整体，由整体到局部，由简单到复杂。层数越多，模型学习的效果也就越精确。")]),t._v(" "),r("p",[t._v("对于语音识别，第一层神经网络可以学习到语言发音的一些音调，后面更深层次的网络可以检测到基本的音素，再到单词信息，再到短语、句子等。")]),t._v(" "),r("p",[t._v("通过这些例子可以看到，随着神经网络的深度加深，模型能学习到更加复杂的问题，功能也更加强大。")]),t._v(" "),r("h2",{attrs:{id:"减少参数数量"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#减少参数数量"}},[t._v("#")]),t._v(" 减少参数数量")]),t._v(" "),r("p",[t._v("虽然通用近似定理说明了，只要一个单隐层前馈网络的神经元足够多，它就能够表示任何函数，但这个网络可能大得不可能实现。因此，使用更深的模型能够减少表示期望函数所需的神经元数量。")]),t._v(" "),r("p",[r("img",{attrs:{src:a(398),alt:"dnn parameters"}})]),t._v(" "),r("p",[t._v("一个直觉上的例子是，比如想要计算 $$x_1 \\text{ XOR } x_2 \\text{ XOR } x_3 \\text{ XOR } \\dots x_n$$（$$x_i \\in { 0, 1 }$$），如果按照上图左边这样用多层网络来计算，一共需要 $$\\log(n)$$ 层，需要的神经元数量不会很多。")]),t._v(" "),r("p",[t._v("但如果只能用一个隐藏层（上图右边），那么就需要枚举所有 $$2^n$$ 种可能的情况，即这个隐藏层需要 $$2^n$$ 个神经元。所需的神经元数量会呈指数级增长。")]),t._v(" "),r("h2",{attrs:{id:"更容易泛化到测试集"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#更容易泛化到测试集"}},[t._v("#")]),t._v(" 更容易泛化到测试集")]),t._v(" "),r("p",[t._v("论文 "),r("a",{attrs:{href:"#goodfellow-2014"}},[t._v("[1]")]),t._v(" 中的实验表明，增加卷积网络层中参数的数量，但不增加它们的深度，在提升测试集性能方面几乎没有效果：")]),t._v(" "),r("p",[r("img",{attrs:{src:a(399),alt:"dnn test"}})]),t._v(" "),r("p",[t._v("同时还可以看到，浅层网络（蓝色线）在参数数量达到 2000 万时就过拟合，而深层网络（红色线）在参数数量超过 6000 万时仍然表现良好。这表明了我们想要学习到的函数应该是许多更简单的函数的复合。")]),t._v(" "),r("h2",{attrs:{id:"引用"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#引用"}},[t._v("#")]),t._v(" 引用")]),t._v(" "),r("div",{attrs:{id:"goodfellow-2014"}}),t._v(" "),r("ol",[r("li",[r("a",{attrs:{href:"https://arxiv.org/pdf/1312.6082.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks"),r("OutboundLink")],1),t._v(". "),r("em",[t._v("Ian J. Goodfellow, et al.")]),t._v(" ICLR 2014.")])])])}),[],!1,null,null,null);e.default=s.exports}}]);